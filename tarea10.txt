Tarea 10: Suite de Tests Integral

**Objetivo**: Tests completos para la nueva funcionalidad.

**Prompt para el agente IA**:
Crear suite de tests en gateway/tests/ con pytest:

Crear conftest.py con fixtures:

pythonimport pytest
from typing import Generator
from fastapi.testclient import TestClient
from sqlmodel import Session, create_engine, SQLModel
from sqlmodel.pool import StaticPool

from app.main import app
from app.db import DatabaseManager

@pytest.fixture(name="session")
def session_fixture() -> Generator[Session, None, None]:
    """Create a new database session for tests."""
    engine = create_engine(
        "sqlite:///:memory:",
        connect_args={"check_same_thread": False},
        poolclass=StaticPool,
    )
    SQLModel.metadata.create_all(engine)
    with Session(engine) as session:
        yield session

@pytest.fixture(name="client")
def client_fixture(session: Session) -> Generator[TestClient, None, None]:
    """Create test client with overridden dependencies."""
    
    def get_session_override():
        return session
    
    app.dependency_overrides[get_session] = get_session_override
    client = TestClient(app)
    yield client
    app.dependency_overrides.clear()

@pytest.fixture
def sample_project(session: Session):
    """Create a sample project for tests."""
    from app.db import ProjectDB
    project = ProjectDB(
        id="test-project",
        name="Test Project",
        path="test-project",
        active=True,
        status="active"
    )
    session.add(project)
    session.commit()
    return project

@pytest.fixture
def sample_task_plan(session: Session, sample_project):
    """Create sample task plan."""
    from app.db import TaskPlanDB
    plan = TaskPlanDB(
        project_id=sample_project.id,
        version=1,
        status="accepted",
        summary="Test plan"
    )
    session.add(plan)
    session.commit()
    return plan

Tests unitarios gateway/tests/test_task_plan_service.py:

pythonimport pytest
import json
from app.services.task_plan_service import TaskPlanService

def test_create_plan(session, sample_project):
    """Test creating a new plan."""
    service = TaskPlanService()
    
    tasks = [
        {
            "code": "T-001",
            "title": "Setup project",
            "description": "Initialize Unity project",
            "dependencies": [],
            "mcp_tools": ["unity"],
            "deliverables": ["ProjectSettings/*"],
            "estimates": {"story_points": 3, "time_hours": 2},
            "priority": 1
        }
    ]
    
    plan = service.create_plan(sample_project.id, tasks)
    
    assert plan.version == 1
    assert plan.status == "proposed"
    assert plan.project_id == sample_project.id

def test_accept_plan(session, sample_task_plan):
    """Test accepting a plan."""
    service = TaskPlanService()
    
    accepted = service.accept_plan(sample_task_plan.id)
    
    assert accepted.status == "accepted"

def test_detect_circular_dependencies():
    """Test circular dependency detection."""
    service = TaskPlanService()
    
    tasks = [
        {"code": "T-001", "dependencies": ["T-002"]},
        {"code": "T-002", "dependencies": ["T-001"]}
    ]
    
    has_circular = service._has_circular_dependencies(tasks)
    assert has_circular == True

Tests de integración gateway/tests/test_integration.py:

pythonimport pytest
from fastapi.testclient import TestClient

def test_plan_generation_flow(client, sample_project):
    """Test complete plan generation flow."""
    
    # 1. Generate plan
    response = client.post(f"/api/v1/plans/generate?projectId={sample_project.id}")
    assert response.status_code == 200
    
    # 2. List plans
    response = client.get(f"/api/v1/plans?projectId={sample_project.id}")
    assert response.status_code == 200
    plans = response.json()
    assert len(plans) > 0
    
    # 3. Accept plan
    plan_id = plans[0]["id"]
    response = client.patch(f"/api/v1/plans/{plan_id}/accept")
    assert response.status_code == 200

def test_task_execution_flow(client, sample_project, sample_task):
    """Test task execution flow."""
    
    # 1. Start task
    response = client.post(f"/api/v1/tasks/{sample_task.id}/start")
    assert response.status_code == 200
    
    # 2. Complete task
    response = client.post(f"/api/v1/tasks/{sample_task.id}/complete")
    assert response.status_code == 200

Coverage report con pytest-cov



*Contexto General de la Arquitectura
Estamos construyendo un sistema de gestión de desarrollo de videojuegos asistido por IA que:
Gestiona proyectos con planes de tareas versionados
Mantiene contexto evolutivo (global y por tarea)
Coordina agentes IA con herramientas MCP
Permite consenso usuario-IA en la planificación
Genera y actualiza contexto automáticamente

*Buenas Prácticas para la Implementación
Base de Datos: Usar siempre `data/gateway.db`
Principios SOLID: Cada componente con responsabilidad única
Idempotencia: Operaciones repetibles sin efectos secundarios
Versionado Semántico: Cambios compatibles hacia atrás
Transacciones Atómicas: Todo o nada en operaciones críticas
Logging Estructurado: Trazabilidad completa con correlation IDs
Eventos: WebSocket para actualizaciones en tiempo real
Tests Unitarios: Mínimo 80% cobertura en lógica de negocio
Documentación API: OpenAPI actualizado con cada cambio